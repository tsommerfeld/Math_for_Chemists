{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence intervals\n",
    "\n",
    "* Confidence intervals regardless of sample size (Student $t$ distribution).\n",
    "* Confidence intervals for proportions.\n",
    "* Useful Jupyter tool: the slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact,fixed,IntSlider\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student $t$ distribution\n",
    "\n",
    "When our sample size $n$ is small, the sample mean $\\bar x$ is normal only if the population is normal. So unless we can assume that our population is normally distributed, we can make no progress anyway.\n",
    "\n",
    "But even then, $s$ is no longer a good estimator for $\\sigma$. For small $n$, $s/\\sqrt{n}$ underestimates the true variation of $\\bar x$, and to correct our formula, we need to replace the normal distribution with the Student $t$ distribution. \n",
    "\n",
    "In practice, a tabled standard $t$ distribution, works very much like a tabled normal distribution.\n",
    "* First we convert $\\bar x$ into a $z$ or $t$ value, and the conversion is actually the same, the difference lies in which distribution are we planning to use: normal ($z$-value) or Student $t$ ($t$-value).\n",
    "* Then we can find the CDF in our table. The CDF is the area to the left of $z$ or $t$.\n",
    "\n",
    "But, but, but ...\n",
    "1. A Student $t$ distribution depends on $n$, so we need a table for each $n$ (or equivalently each $df$).\n",
    "2. Normally, we want the inverse function of the CDF, that is, we know the propability - in confidence interval language the area $\\alpha/2$ - and we would like to find the corresponding $z$ or $t$ value. To compute the inverse of the CDF with a table, we work the table \"backwards\". However, we can, of course, tabulate the inverse function, and Fig. 7.1.6 lists results for some frequently needed values of $\\alpha$.\n",
    "\n",
    "### Task\n",
    "\n",
    "Make sure you understand that Fig 7.1.5 lists the CDF of a normal distribution, while Fig. 7.1.6 lists the inverse of the Student $t$ distribution with the limiting nomal distribution in the last line ($df=\\infty$ line). Identify all entries in the $df=\\infty$ line in Fig 7.1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names of functions you already know.\n",
    "\n",
    "First of all, for the time being let's stick to *standard* functions: *Standard* implies a mean of $0$ and a standard deviation of $1$. For example, a standard normal distribution or a standard Student $t$ distribution. Instead of $X$ and $x$, we use either $Z$ and $z$ or $T$ or $t$ to indicate that we deal with a standard normal or standard $t$ distribution. \n",
    "\n",
    "We already used a CDF table as in Fig 7.1.5 in various examples. In general, a CDF is a function that takes an $x$, $z$, or $t$ value as input, and returns the probability to the left of that value, in other words, the left area: $P_{left} = \\mathrm{CDF}(z)$.  \n",
    "\n",
    "In addition, we used three related functions without giving them an explicit name:\n",
    "1. The probability or area to the right of $z$: $P_{right}=1 - \\mathrm{CDF}(z)$.\n",
    "2. The inverse of the CDF: A function that takes the left area as an input and returns the corresponding $z$ or $t$ value.\n",
    "3. The inverse of $1-\\mathrm{CDF}$: A function that takes the right area as an input and returns the corresponding $z$ or $t$ value.\n",
    "\n",
    "These four functions are closely related, and when practicing with a normal distribution we saw that the table codes both the CDF (working the table \"forward\") and its inverse (working the table \"backward\"), and that the left area functions can be computed using \"left area plus right area equals $1$\".\n",
    "\n",
    "Still, you will not be surprised to hear that all four functions have names, and that you the names come in handy for understanding the literature and for calling Python functions.\n",
    "1. The right probability is called *survival function* $\\mathrm{SF} = 1 - \\mathrm{CDF}$. \n",
    "2. The inverse of the CDF is called *percent point function*, PPF. For example, for a normal distribution, PPF($0.1$) will give you the $z$-value of the $10$%-ile. \n",
    "3. The inverse of the survival function has no specical name but is abivated ISF. For example, ISF($0.1$) looks for a right tail containg $10$%, the $90$%-ile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scipy` implementation\n",
    "\n",
    "A RV having a normal distribution: `rv=scipy.stats.norm`\n",
    "\n",
    "A RV having a Student $t$ distribution: `rv=scipy.stats.t`\n",
    "\n",
    "The four functions:\n",
    "\n",
    "Function| Normal | Student t |\n",
    "---|---|---|\n",
    "CDF | `rv.cdf(z)` | `rv.cdf(t, df)`\n",
    "SF  | `rv.sf(z)` | `rv.sf(t, df)`\n",
    "PPF | `rv.ppf(P_left)` | `rv.ppf(P_left, df)`\n",
    "ISF | `rv.isf(P_right)` | `rv.isf(P_right, df)`\n",
    "\n",
    "As you can see, straightforward if you know the names.\n",
    "\n",
    "Now we are ready to compute a confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the distribution\n",
    "rv=scipy.stats.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sample \n",
    "# normally we would have data in a pandas column and compute xbar and s\n",
    "n=5\n",
    "xbar=0.678\n",
    "s=0.234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence level we want to compute; confidence_level = 1 - alpha\n",
    "conf_level=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval: 0.678 ± 0.482  or  (0.196, 1.160)\n"
     ]
    }
   ],
   "source": [
    "# compute the confidence interval\n",
    "# we use ISF because we want a right tail of alpha/2 \n",
    "df = n-1\n",
    "alpha = 1-conf_level\n",
    "t_alpha_over_2 = rv.isf(alpha/2, df)\n",
    "E = t_alpha_over_2 * s / np.sqrt(n)\n",
    "print(f'Confidence interval: {xbar:.3f} ± {E:.3f}  or  ({xbar-E:.3f}, {xbar+E:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task \n",
    "\n",
    "Solve problem 7.2.18 using a $t$-table. Then check your work using the Jupyter cells above. \n",
    "\n",
    "## Sliders\n",
    "\n",
    "Often, I find myself playing with cells such as the example above: an output (here $E$ is computed from multiple inputs ($n$, mean, $s$, confidence level). I do this either to  understand how the output behaves when the input changes, or to optimize the input for a particular purpose. \n",
    "\n",
    "As long as I have to scroll up, change the input, and re-execute the Jupyter cells less than 10 times or so that is fine, but beyond that this procedure becomes tiresome and clumsy.\n",
    "\n",
    "A better option are ***sliders***. Sliders enable you to change a variable using the mouse, and instantaneously use the new values in a compution, which you can simply print or even plot.\n",
    "\n",
    "Example: Slider for sample size $n$ from $5$ to $50$, step $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0438cb6181f436088fad4aeb322d8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', max=50, min=5), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(n=(5, 50, 1))\n",
    "def print_sample_size(n=10):\n",
    "    print(f'sample size is {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Modify the `print_sample_size` function to also print the degrees-of-freedom $df$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second example: two sliders, one for $n$, one for the confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed859623d5b447afb75b50e094399d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', max=50, min=5), FloatSlider(value=95.0, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(n=(5, 50, 1), conf_level=(90, 99.9, 0.1))\n",
    "def sampling(n=10, conf_level=95):\n",
    "    print(f'sample size is {n}, confidence level is {conf_level:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the Student $t$ confidence interval calculation from above with sliders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9c4dd532bb4312a19bfc8d3877e187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', max=50, min=5), FloatSlider(value=95.0, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(n=(5, 50, 1), conf_level=(90, 99.9, 0.1))\n",
    "def sampling(n=10, conf_level=95):\n",
    "    xbar=0.678\n",
    "    s=0.234\n",
    "    df = n-1\n",
    "    alpha = 1-conf_level/100\n",
    "    t_alpha_over_2 = rv.isf(alpha/2, df)\n",
    "    E = t_alpha_over_2 * s / np.sqrt(n)\n",
    "    print(f'Confidence interval: {xbar:.3f} ± {E:.3f}  or  ({xbar-E:.3f}, {xbar+E:.3f})')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Is the margin or error more susceptiple to the the sample size or to the confidence interval? In what sense?\n",
    "\n",
    "### Task\n",
    "\n",
    "Proportion sliders: \n",
    "* Copy-and-paste the last Jupyter cell to the cell below. \n",
    "* Modify the sliders for input of $n$ and $\\hat p$. \n",
    "* Check whether the sample is \"large\", and either print \"large sample\" or \"too small sample\".\n",
    "* Compute the margin of error $E$ (careful, now we need a normal RV).\n",
    "* Play with your sliders: For which proportions are small samples sufficient? For which proportions do we need rather large samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the distribution\n",
    "rv=scipy.stats.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643ee475ca80460fa7a48f8d84135132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=40, description='n', min=5), FloatSlider(value=0.4, description='p_hat',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(n=(5, 100, 1), p_hat=(0.02, 0.988, 0.02), conf_level=(90, 99.9, 0.1))\n",
    "def sampling(n=40, p_hat=0.4, conf_level=95):\n",
    "    sigma=np.sqrt(p_hat*(1-p_hat)/n)\n",
    "    low=p_hat-3*sigma\n",
    "    high=p_hat+3*sigma\n",
    "    print(f'3-sigma interval: [{low:.3f}, {high:.3f}]')\n",
    "    if low > 0 and high < 1:\n",
    "        print('large sample')\n",
    "    else:\n",
    "        print('too small sample')\n",
    "    alpha = 1-conf_level/100\n",
    "    z_alpha_over_2 = rv.isf(alpha/2)\n",
    "    E = z_alpha_over_2 * sigma\n",
    "    print(f'Confidence interval: {p_hat:.3f} ± {E:.3f}  or  ({p_hat-E:.3f}, {p_hat+E:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
